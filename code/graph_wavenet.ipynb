{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class nconv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(nconv,self).__init__()\n",
    "\n",
    "    def forward(self,x, A):\n",
    "        x = torch.einsum('ncvl,vw->ncwl',(x,A))\n",
    "        return x.contiguous()\n",
    "\n",
    "class linear(nn.Module):\n",
    "    def __init__(self,c_in,c_out):\n",
    "        super(linear,self).__init__()\n",
    "        self.mlp = torch.nn.Conv2d(c_in, c_out, kernel_size=(1, 1), padding=(0,0), stride=(1,1), bias=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class gcn(nn.Module):\n",
    "    def __init__(self,c_in,c_out,dropout,support_len=3,order=2):\n",
    "        super(gcn,self).__init__()\n",
    "        self.nconv = nconv()\n",
    "        c_in = (order*support_len+1)*c_in\n",
    "        self.mlp = linear(c_in,c_out)\n",
    "        self.dropout = dropout\n",
    "        self.order = order\n",
    "\n",
    "    def forward(self,x,support):\n",
    "        out = [x]\n",
    "        for a in support:\n",
    "            x1 = self.nconv(x,a)\n",
    "            out.append(x1)\n",
    "            for k in range(2, self.order + 1):\n",
    "                x2 = self.nconv(x1,a)\n",
    "                out.append(x2)\n",
    "                x1 = x2\n",
    "\n",
    "        h = torch.cat(out,dim=1)\n",
    "        h = self.mlp(h)\n",
    "        h = F.dropout(h, self.dropout, training=self.training)\n",
    "        return h\n",
    "\n",
    "\n",
    "class gwnet(nn.Module):\n",
    "    def __init__(self, device, num_nodes, dropout=0.3, supports=None, gcn_bool=True, addaptadj=True, aptinit=None, in_dim=2,\n",
    "    out_dim=12,residual_channels=32,dilation_channels=32,skip_channels=256,end_channels=512,kernel_size=2,blocks=4,layers=2):\n",
    "        super(gwnet, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.blocks = blocks\n",
    "        self.layers = layers\n",
    "        self.gcn_bool = gcn_bool\n",
    "        self.addaptadj = addaptadj\n",
    "        self.num_nodes = num_nodes\n",
    "\n",
    "        self.filter_convs = nn.ModuleList()\n",
    "        self.gate_convs = nn.ModuleList()\n",
    "        self.residual_convs = nn.ModuleList()\n",
    "        self.skip_convs = nn.ModuleList()\n",
    "        self.bn = nn.ModuleList()\n",
    "        self.gconv = nn.ModuleList()\n",
    "\n",
    "        self.start_conv = nn.Conv2d(in_channels=in_dim,\n",
    "                                    out_channels=residual_channels,\n",
    "                                    kernel_size=(1,1))\n",
    "        self.supports = supports\n",
    "\n",
    "        receptive_field = 1\n",
    "\n",
    "        self.supports_len = 0\n",
    "        if supports is not None:\n",
    "            self.supports_len += len(supports)\n",
    "\n",
    "        if gcn_bool and addaptadj:\n",
    "            if aptinit is None:\n",
    "                if supports is None:\n",
    "                    self.supports = []\n",
    "                self.nodevec1 = nn.Parameter(torch.randn(num_nodes, 10).to(device), requires_grad=True).to(device)\n",
    "                self.nodevec2 = nn.Parameter(torch.randn(10, num_nodes).to(device), requires_grad=True).to(device)\n",
    "                self.supports_len +=1\n",
    "            else:\n",
    "                if supports is None:\n",
    "                    self.supports = []\n",
    "                m, p, n = torch.svd(aptinit)  # aptinit & m & n: [node, node], p: [node]\n",
    "                initemb1 = torch.mm(m[:, :10], torch.diag(p[:10] ** 0.5)) # 10是为了与随机embedding为10保持一致\n",
    "                initemb2 = torch.mm(torch.diag(p[:10] ** 0.5), n[:, :10].t())\n",
    "                self.nodevec1 = nn.Parameter(initemb1, requires_grad=True).to(device)\n",
    "                self.nodevec2 = nn.Parameter(initemb2, requires_grad=True).to(device)\n",
    "                self.supports_len += 1\n",
    "\n",
    "\n",
    "        for b in range(blocks):\n",
    "            additional_scope = kernel_size - 1\n",
    "            new_dilation = 1\n",
    "            for i in range(layers):\n",
    "                # dilated convolutions\n",
    "                self.filter_convs.append(nn.Conv2d(in_channels=residual_channels,\n",
    "                                                   out_channels=dilation_channels,\n",
    "                                                   kernel_size=(1,kernel_size),dilation=new_dilation))\n",
    "\n",
    "                self.gate_convs.append(nn.Conv1d(in_channels=residual_channels,\n",
    "                                                 out_channels=dilation_channels,\n",
    "                                                 kernel_size=(1, kernel_size), dilation=new_dilation))\n",
    "\n",
    "                # 1x1 convolution for residual connection\n",
    "                self.residual_convs.append(nn.Conv1d(in_channels=dilation_channels,\n",
    "                                                     out_channels=residual_channels,\n",
    "                                                     kernel_size=(1, 1)))\n",
    "\n",
    "                # 1x1 convolution for skip connection\n",
    "                self.skip_convs.append(nn.Conv1d(in_channels=dilation_channels,\n",
    "                                                 out_channels=skip_channels,\n",
    "                                                 kernel_size=(1, 1)))\n",
    "                self.bn.append(nn.BatchNorm2d(residual_channels))\n",
    "                new_dilation *=2\n",
    "                receptive_field += additional_scope\n",
    "                additional_scope *= 2\n",
    "                if self.gcn_bool:\n",
    "                    self.gconv.append(gcn(dilation_channels,residual_channels,dropout,support_len=self.supports_len))\n",
    "\n",
    "\n",
    "\n",
    "        self.end_conv_1 = nn.Conv2d(in_channels=skip_channels,\n",
    "                                  out_channels=end_channels,\n",
    "                                  kernel_size=(1,1),\n",
    "                                  bias=True)\n",
    "\n",
    "        self.end_conv_2 = nn.Conv2d(in_channels=end_channels,\n",
    "                                    out_channels=out_dim,\n",
    "                                    kernel_size=(1,1),\n",
    "                                    bias=True)\n",
    "\n",
    "        self.receptive_field = receptive_field\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input: [bs, 2, node, 12]\n",
    "        in_len = input.size(3) \n",
    "        if in_len<self.receptive_field:  # 13\n",
    "            x = nn.functional.pad(input,(self.receptive_field-in_len,0,0,0)) # [bs, 2, node, 13]\n",
    "        else:\n",
    "            x = input\n",
    "        x = self.start_conv(x) # [bs, 2, node, 13]\n",
    "        skip = 0\n",
    "\n",
    "        # calculate the current adaptive adj matrix once per iteration\n",
    "        new_supports = None\n",
    "        if self.gcn_bool and self.addaptadj and self.supports is not None:\n",
    "            adp = F.softmax(F.relu(torch.mm(self.nodevec1, self.nodevec2)), dim=1)\n",
    "            new_supports = self.supports + [adp]\n",
    "\n",
    "        # WaveNet layers\n",
    "        for i in range(self.blocks * self.layers):\n",
    "\n",
    "            #            |----------------------------------------|     *residual*\n",
    "            #            |                                        |\n",
    "            #            |    |-- conv -- tanh --|                |\n",
    "            # -> dilate -|----|                  * ----|-- 1x1 -- + -->\t*input*\n",
    "            #                 |-- conv -- sigm --|     |\n",
    "            #                                         1x1\n",
    "            #                                          |\n",
    "            # ---------------------------------------> + ------------->\t*skip*\n",
    "\n",
    "            #(dilation, init_dilation) = self.dilations[i]\n",
    "\n",
    "            #residual = dilation_func(x, dilation, init_dilation, i)\n",
    "            residual = x\n",
    "            # dilated convolution\n",
    "            filter = self.filter_convs[i](residual)\n",
    "            filter = torch.tanh(filter)\n",
    "            gate = self.gate_convs[i](residual)\n",
    "            gate = torch.sigmoid(gate)\n",
    "            x = filter * gate\n",
    "\n",
    "            # parametrized skip connection\n",
    "\n",
    "            s = x\n",
    "            s = self.skip_convs[i](s)\n",
    "            try:\n",
    "                skip = skip[:, :, :,  -s.size(3):] # 当前步的skip加上上一步的skip\n",
    "            except:\n",
    "                skip = 0\n",
    "            skip = s + skip\n",
    "\n",
    "\n",
    "            if self.gcn_bool and self.supports is not None:\n",
    "                if self.addaptadj:\n",
    "                    x = self.gconv[i](x, new_supports)\n",
    "                else:\n",
    "                    x = self.gconv[i](x,self.supports)\n",
    "            else:\n",
    "                x = self.residual_convs[i](x)\n",
    "\n",
    "            x = x + residual[:, :, :, -x.size(3):] # 当前步的X加上上一步的x\n",
    "\n",
    "\n",
    "            x = self.bn[i](x)\n",
    "        \n",
    "        x = F.relu(skip) # [bs, skip_channels, node, 1]\n",
    "        x = F.relu(self.end_conv_1(x))\n",
    "        x = self.end_conv_2(x)\n",
    "        return x.view(-1, self.num_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from scipy.sparse import linalg\n",
    "\n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self, xs, ys, batch_size, pad_with_last_sample=True):\n",
    "        \"\"\"\n",
    "        :param xs:\n",
    "        :param ys:\n",
    "        :param batch_size:\n",
    "        :param pad_with_last_sample: pad with the last sample to make number of samples divisible to batch_size.\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.current_ind = 0\n",
    "        if pad_with_last_sample:\n",
    "            num_padding = (batch_size - (len(xs) % batch_size)) % batch_size\n",
    "            x_padding = np.repeat(xs[-1:], num_padding, axis=0)\n",
    "            y_padding = np.repeat(ys[-1:], num_padding, axis=0)\n",
    "            xs = np.concatenate([xs, x_padding], axis=0)\n",
    "            ys = np.concatenate([ys, y_padding], axis=0)\n",
    "        self.size = len(xs)\n",
    "        self.num_batch = int(self.size // self.batch_size)\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "\n",
    "    def shuffle(self):\n",
    "        permutation = np.random.permutation(self.size)\n",
    "        xs, ys = self.xs[permutation], self.ys[permutation]\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "\n",
    "    def get_iterator(self):\n",
    "        self.current_ind = 0\n",
    "\n",
    "        def _wrapper():\n",
    "            while self.current_ind < self.num_batch:\n",
    "                start_ind = self.batch_size * self.current_ind\n",
    "                end_ind = min(self.size, self.batch_size * (self.current_ind + 1))\n",
    "                x_i = self.xs[start_ind: end_ind, ...]\n",
    "                y_i = self.ys[start_ind: end_ind, ...]\n",
    "                yield (x_i, y_i)\n",
    "                self.current_ind += 1\n",
    "\n",
    "        return _wrapper()\n",
    "\n",
    "class StandardScaler():\n",
    "    \"\"\"\n",
    "    Standard the input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def transform(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return (data * self.std) + self.mean\n",
    "\n",
    "\n",
    "\n",
    "def sym_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).astype(np.float32).todense()\n",
    "\n",
    "def asym_adj(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1)).flatten()\n",
    "    d_inv = np.power(rowsum, -1).flatten()\n",
    "    d_inv[np.isinf(d_inv)] = 0.\n",
    "    d_mat= sp.diags(d_inv)\n",
    "    return d_mat.dot(adj).astype(np.float32).todense()\n",
    "\n",
    "def calculate_normalized_laplacian(adj):\n",
    "    \"\"\"\n",
    "    # L = D^-1/2 (D-A) D^-1/2 = I - D^-1/2 A D^-1/2\n",
    "    # D = diag(A 1)\n",
    "    :param adj:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    d = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(d, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    normalized_laplacian = sp.eye(adj.shape[0]) - adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "    return normalized_laplacian\n",
    "\n",
    "def calculate_scaled_laplacian(adj_mx, lambda_max=2, undirected=True):\n",
    "    if undirected:\n",
    "        adj_mx = np.maximum.reduce([adj_mx, adj_mx.T])\n",
    "    L = calculate_normalized_laplacian(adj_mx)\n",
    "    if lambda_max is None:\n",
    "        lambda_max, _ = linalg.eigsh(L, 1, which='LM')\n",
    "        lambda_max = lambda_max[0]\n",
    "    L = sp.csr_matrix(L)\n",
    "    M, _ = L.shape\n",
    "    I = sp.identity(M, format='csr', dtype=L.dtype)\n",
    "    L = (2 / lambda_max * L) - I\n",
    "    return L.astype(np.float32).todense()\n",
    "\n",
    "def masked_mse(preds, labels, null_val=np.nan):\n",
    "    if np.isnan(null_val):\n",
    "        mask = ~torch.isnan(labels)\n",
    "    else:\n",
    "        mask = (labels!=null_val)\n",
    "    mask = mask.float()\n",
    "    mask /= torch.mean((mask))\n",
    "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
    "    loss = (preds-labels)**2\n",
    "    loss = loss * mask\n",
    "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "def masked_rmse(preds, labels, null_val=np.nan):\n",
    "    return torch.sqrt(masked_mse(preds=preds, labels=labels, null_val=null_val))\n",
    "\n",
    "\n",
    "def masked_mae(preds, labels, null_val=np.nan):\n",
    "    if np.isnan(null_val):\n",
    "        mask = ~torch.isnan(labels)\n",
    "    else:\n",
    "        mask = (labels!=null_val)\n",
    "    mask = mask.float()\n",
    "    mask /=  torch.mean((mask))\n",
    "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
    "    loss = torch.abs(preds-labels)\n",
    "    loss = loss * mask\n",
    "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "\n",
    "def masked_mape(preds, labels, null_val=np.nan):\n",
    "    if np.isnan(null_val):\n",
    "        mask = ~torch.isnan(labels)\n",
    "    else:\n",
    "        mask = (labels!=null_val)\n",
    "    mask = mask.float()\n",
    "    mask /=  torch.mean((mask))\n",
    "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
    "    loss = torch.abs(preds-labels)/labels\n",
    "    loss = loss * mask\n",
    "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "\n",
    "def metric(pred, real):\n",
    "    mae = masked_mae(pred,real,0.0).item()\n",
    "    mape = masked_mape(pred,real,0.0).item()\n",
    "    rmse = masked_rmse(pred,real,0.0).item()\n",
    "    return mae,mape,rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class trainer():\n",
    "    def __init__(self, scaler, in_dim, seq_length, num_nodes, nhid , dropout, lrate, wdecay, device, supports, gcn_bool, addaptadj, aptinit):\n",
    "        self.model = gwnet(device, num_nodes, dropout, supports=supports, gcn_bool=gcn_bool, addaptadj=addaptadj, \n",
    "                           aptinit=aptinit, in_dim=in_dim, out_dim=1, residual_channels=nhid,\n",
    "                           dilation_channels=nhid, skip_channels=nhid * 8, end_channels=nhid * 16,\n",
    "                           kernel_size=4, blocks=8, layers=2)\n",
    "        self.model.to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lrate, weight_decay=wdecay)\n",
    "        self.loss = masked_mae\n",
    "        self.scaler = scaler\n",
    "        self.clip = 5\n",
    "\n",
    "    def train(self, input, real_val):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        input = nn.functional.pad(input,(1,0,0,0))\n",
    "        output = self.model(input)\n",
    "        #output = output.transpose(1,3)\n",
    "        #output = [batch_size,12,num_nodes,1]\n",
    "        #eal = torch.unsqueeze(real_val,dim=1)\n",
    "        predict = self.scaler.inverse_transform(output)\n",
    "\n",
    "        loss = self.loss(predict, real_val, 0.0)\n",
    "        loss.backward()\n",
    "        if self.clip is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n",
    "        self.optimizer.step()\n",
    "        mape = masked_mape(predict,real_val,10.0).item()\n",
    "        rmse = masked_rmse(predict,real_val,0.0).item()\n",
    "        return loss.item(),mape,rmse\n",
    "\n",
    "    def eval(self, input, real_val):\n",
    "        self.model.eval()\n",
    "        input = nn.functional.pad(input,(1,0,0,0))\n",
    "        output = self.model(input)\n",
    "        #output = output.transpose(1,3)\n",
    "        #output = [batch_size,12,num_nodes,1]\n",
    "        #real = torch.unsqueeze(real_val,dim=1)\n",
    "        predict = self.scaler.inverse_transform(output)\n",
    "        loss = self.loss(predict, real_val, 0.0)\n",
    "        mape = masked_mape(predict,real_val,0.0).item()\n",
    "        rmse = masked_rmse(predict,real_val,0.0).item()\n",
    "        return loss.item(),mape,rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true1, y_pred1 = np.array(y_true).flatten(), np.array(y_pred).flatten()\n",
    "    y_true1, y_pred1 = y_true1[np.where(y_true1 > 10)[0]], y_pred1[np.where(y_true1 > 10)[0]]\n",
    "    return np.mean(np.abs((y_true1 - y_pred1) / y_true1)) * 100\n",
    "\n",
    "def compute_me(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_true[np.where(y_true > 10)[0]], y_pred[np.where(y_true > 10)[0]])\n",
    "    return mae, rmse, mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## TaxiSZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(654, 2)\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import numpy\n",
    "\n",
    "X_train_sz = np.load('./data/taxi_sz/X_train20.npy').astype('float32')\n",
    "X_val_sz = np.load('./data/taxi_sz/X_val20.npy').astype('float32')\n",
    "X_test_sz = np.load('./data/taxi_sz/X_test20.npy').astype('float32')\n",
    "y_train_sz = np.load('./data/taxi_sz/y_train20.npy').astype('float32')\n",
    "y_val_sz = np.load('./data/taxi_sz/y_val20.npy').astype('float32')\n",
    "y_test_sz = np.load('./data/taxi_sz/y_test20.npy').astype('float32')\n",
    "\n",
    "edges_in_sz = np.load('./data/taxi_sz/edges_in_sz.npy')\n",
    "print(edges_in_sz.shape)\n",
    "\n",
    "high_similar_poi_sz = pd.read_csv('./data/taxi_sz/high_similar_poi.csv')\n",
    "high_similar_poi_sz.head(2)\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3672, 1, 101, 24) (3672, 101) (504, 1, 101, 24) (504, 101)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = X_train_sz.transpose(1,0,2)[:,np.newaxis,:,:], y_train_sz.T\n",
    "X_val, y_val = X_val_sz.transpose(1,0,2)[:,np.newaxis,:,:], y_val_sz.T\n",
    "X_test, y_test = X_test_sz.transpose(1,0,2)[:,np.newaxis,:,:], y_test_sz.T\n",
    "\n",
    "scaler = StandardScaler(np.mean(X_train.flatten()), np.std(X_train.flatten()))\n",
    "\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_val_std = scaler.transform(X_val)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "print(X_train_std.shape, y_train.shape, X_test_std.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "adj_mx = np.zeros((101, 101))\n",
    "\n",
    "for i in range(101): \n",
    "    adj_mx[i,i] = 1\n",
    "    \n",
    "for i in range(len(edges_in_sz)):\n",
    "    adj_mx[edges_in_sz[i,0], edges_in_sz[i,1]] = 1\n",
    "    \n",
    "for i in range(len(high_similar_poi_sz)):\n",
    "    adj_mx[high_similar_poi_sz['i'].iloc[i], high_similar_poi_sz['j'].iloc[i]] = 1\n",
    "    \n",
    "adj_mx = torch.FloatTensor(adj_mx).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "device=torch.device('cuda')\n",
    "trainloader = DataLoader(X_train_std, y_train, 1)\n",
    "valloader = DataLoader(X_val_std, y_val, 1)\n",
    "testloader = DataLoader(X_test_std, y_test, 1)\n",
    "    \n",
    "engine = trainer(scaler=scaler, in_dim=1, seq_length=24, num_nodes=101, nhid=32, dropout=0.5, lrate=0.01, wdecay=0.0001, \n",
    "    device=device, supports=[adj_mx], gcn_bool=True, addaptadj=True, aptinit=adj_mx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "Iter: 000, Train Loss: 380.7150, Train MAPE: 33.8767, Train RMSE: 387.4974\n",
      "Iter: 050, Train Loss: 8.6841, Train MAPE: inf, Train RMSE: 11.6685\n",
      "Iter: 100, Train Loss: 12.1201, Train MAPE: inf, Train RMSE: 18.3169\n",
      "Iter: 150, Train Loss: 7.7791, Train MAPE: inf, Train RMSE: 12.4082\n",
      "Iter: 200, Train Loss: 8.5986, Train MAPE: inf, Train RMSE: 16.0842\n",
      "Iter: 250, Train Loss: 9.2662, Train MAPE: inf, Train RMSE: 15.0268\n",
      "Iter: 300, Train Loss: 6.2013, Train MAPE: inf, Train RMSE: 8.6370\n",
      "Iter: 350, Train Loss: 10.1335, Train MAPE: inf, Train RMSE: 16.0608\n",
      "Iter: 400, Train Loss: 31.1759, Train MAPE: inf, Train RMSE: 39.9931\n",
      "Iter: 450, Train Loss: 7.3248, Train MAPE: inf, Train RMSE: 12.9246\n",
      "Iter: 500, Train Loss: 4.1576, Train MAPE: inf, Train RMSE: 6.5574\n",
      "Iter: 550, Train Loss: 11.5257, Train MAPE: 0.6091, Train RMSE: 19.7673\n",
      "Iter: 600, Train Loss: 6.8442, Train MAPE: inf, Train RMSE: 10.5598\n",
      "Iter: 650, Train Loss: 8.0521, Train MAPE: inf, Train RMSE: 12.0541\n",
      "Iter: 700, Train Loss: 9.1644, Train MAPE: inf, Train RMSE: 12.5276\n",
      "Iter: 750, Train Loss: 5.2675, Train MAPE: inf, Train RMSE: 7.0380\n",
      "Iter: 800, Train Loss: 7.6718, Train MAPE: inf, Train RMSE: 9.9538\n",
      "Iter: 850, Train Loss: 8.7226, Train MAPE: inf, Train RMSE: 12.8193\n",
      "Iter: 900, Train Loss: 6.8434, Train MAPE: inf, Train RMSE: 12.2008\n",
      "Iter: 950, Train Loss: 5.8250, Train MAPE: 0.4137, Train RMSE: 7.7595\n",
      "Iter: 1000, Train Loss: 10.2815, Train MAPE: inf, Train RMSE: 16.6263\n",
      "Iter: 1050, Train Loss: 7.0376, Train MAPE: inf, Train RMSE: 10.2927\n",
      "Iter: 1100, Train Loss: 21.4674, Train MAPE: inf, Train RMSE: 43.0820\n",
      "Iter: 1150, Train Loss: 5.4950, Train MAPE: inf, Train RMSE: 7.9267\n",
      "Iter: 1200, Train Loss: 5.7385, Train MAPE: inf, Train RMSE: 7.2769\n",
      "Iter: 1250, Train Loss: 10.0106, Train MAPE: inf, Train RMSE: 15.8453\n",
      "Iter: 1300, Train Loss: 8.2173, Train MAPE: inf, Train RMSE: 11.2450\n",
      "Iter: 1350, Train Loss: 8.4237, Train MAPE: inf, Train RMSE: 12.7552\n",
      "Iter: 1400, Train Loss: 2.6724, Train MAPE: inf, Train RMSE: 4.4693\n",
      "Iter: 1450, Train Loss: 7.7604, Train MAPE: inf, Train RMSE: 10.5946\n",
      "Iter: 1500, Train Loss: 7.3936, Train MAPE: inf, Train RMSE: 11.2872\n",
      "Iter: 1550, Train Loss: 3.9594, Train MAPE: inf, Train RMSE: 6.4012\n",
      "Iter: 1600, Train Loss: 3.6477, Train MAPE: inf, Train RMSE: 6.4459\n",
      "Iter: 1650, Train Loss: 7.6190, Train MAPE: inf, Train RMSE: 13.0766\n",
      "Iter: 1700, Train Loss: 4.5034, Train MAPE: inf, Train RMSE: 6.3687\n",
      "Iter: 1750, Train Loss: 5.2496, Train MAPE: inf, Train RMSE: 7.5961\n",
      "Iter: 1800, Train Loss: 11.5041, Train MAPE: inf, Train RMSE: 19.1823\n",
      "Iter: 1850, Train Loss: 8.4012, Train MAPE: inf, Train RMSE: 14.4161\n",
      "Iter: 1900, Train Loss: 7.4424, Train MAPE: 0.3641, Train RMSE: 11.4594\n",
      "Iter: 1950, Train Loss: 12.2624, Train MAPE: inf, Train RMSE: 18.1986\n",
      "Iter: 2000, Train Loss: 8.0859, Train MAPE: inf, Train RMSE: 11.3989\n",
      "Iter: 2050, Train Loss: 11.7562, Train MAPE: inf, Train RMSE: 20.8425\n",
      "Iter: 2100, Train Loss: 8.6513, Train MAPE: inf, Train RMSE: 14.6974\n",
      "Iter: 2150, Train Loss: 29.9806, Train MAPE: inf, Train RMSE: 59.6373\n",
      "Iter: 2200, Train Loss: 3.8618, Train MAPE: inf, Train RMSE: 5.1229\n",
      "Iter: 2250, Train Loss: 7.1003, Train MAPE: 0.5439, Train RMSE: 10.3254\n",
      "Iter: 2300, Train Loss: 6.3256, Train MAPE: inf, Train RMSE: 10.5978\n",
      "Iter: 2350, Train Loss: 9.9722, Train MAPE: inf, Train RMSE: 13.8946\n",
      "Iter: 2400, Train Loss: 6.8285, Train MAPE: inf, Train RMSE: 8.5532\n",
      "Iter: 2450, Train Loss: 9.6803, Train MAPE: 0.5119, Train RMSE: 14.9404\n",
      "Iter: 2500, Train Loss: 6.5415, Train MAPE: inf, Train RMSE: 9.0887\n",
      "Iter: 2550, Train Loss: 6.4922, Train MAPE: inf, Train RMSE: 9.5810\n",
      "Iter: 2600, Train Loss: 7.7690, Train MAPE: inf, Train RMSE: 11.8458\n",
      "Iter: 2650, Train Loss: 6.8707, Train MAPE: inf, Train RMSE: 9.7758\n",
      "Iter: 2700, Train Loss: 6.6824, Train MAPE: inf, Train RMSE: 11.6712\n",
      "Iter: 2750, Train Loss: 3.3017, Train MAPE: inf, Train RMSE: 5.1633\n",
      "Iter: 2800, Train Loss: 5.9105, Train MAPE: inf, Train RMSE: 8.2603\n",
      "Iter: 2850, Train Loss: 8.7180, Train MAPE: 0.9322, Train RMSE: 10.6715\n",
      "Iter: 2900, Train Loss: 7.1213, Train MAPE: inf, Train RMSE: 10.4277\n",
      "Iter: 2950, Train Loss: 4.8512, Train MAPE: inf, Train RMSE: 8.8575\n",
      "Iter: 3000, Train Loss: 7.8973, Train MAPE: inf, Train RMSE: 9.4026\n",
      "Iter: 3050, Train Loss: 11.3407, Train MAPE: inf, Train RMSE: 17.0701\n",
      "Iter: 3100, Train Loss: 6.6294, Train MAPE: inf, Train RMSE: 10.0758\n",
      "Iter: 3150, Train Loss: 6.8849, Train MAPE: 0.3899, Train RMSE: 10.8498\n",
      "Iter: 3200, Train Loss: 12.7559, Train MAPE: inf, Train RMSE: 15.7547\n",
      "Iter: 3250, Train Loss: 20.0703, Train MAPE: 0.6172, Train RMSE: 37.4847\n",
      "Iter: 3300, Train Loss: 6.8092, Train MAPE: inf, Train RMSE: 9.5424\n",
      "Iter: 3350, Train Loss: 10.6720, Train MAPE: inf, Train RMSE: 16.8855\n",
      "Iter: 3400, Train Loss: 7.3382, Train MAPE: inf, Train RMSE: 11.8920\n",
      "Iter: 3450, Train Loss: 8.2237, Train MAPE: inf, Train RMSE: 9.6307\n",
      "Iter: 3500, Train Loss: 8.3457, Train MAPE: inf, Train RMSE: 10.3984\n",
      "Iter: 3550, Train Loss: 5.6349, Train MAPE: inf, Train RMSE: 7.1726\n",
      "Iter: 3600, Train Loss: 9.1554, Train MAPE: inf, Train RMSE: 15.5459\n",
      "Iter: 3650, Train Loss: 10.3069, Train MAPE: inf, Train RMSE: 15.8944\n",
      "Epoch: 001, Inference Time: 33.0158 secs\n",
      "Epoch: 001, Train Loss: 11.3843, Train MAPE: inf, Train RMSE: 18.2968, Valid Loss: 9.5440, Valid MAPE: 1.2245, Valid RMSE: 13.2774, Training Time: 970.4954/epoch\n",
      "Average Training Time: 970.4954 secs/epoch\n",
      "Average Inference Time: 33.0158 secs\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b4902ca38c50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m \u001b[0mrealy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y_test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_loader'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"start training...\",flush=True)\n",
    "his_loss =[]\n",
    "val_time = []\n",
    "train_time = []\n",
    "for i in range(1,2):\n",
    "    #if i % 10 == 0:\n",
    "        #lr = max(0.000002,args.learning_rate * (0.1 ** (i // 10)))\n",
    "        #for g in engine.optimizer.param_groups:\n",
    "            #g['lr'] = lr\n",
    "    train_loss = []\n",
    "    train_mape = []\n",
    "    train_rmse = []\n",
    "    t1 = time.time()\n",
    "    trainloader.shuffle()\n",
    "    for iter, (x, y) in enumerate(trainloader.get_iterator()):\n",
    "        trainx = torch.Tensor(x).to(device)\n",
    "        trainy = torch.Tensor(y).to(device)\n",
    "        metrics = engine.train(trainx, trainy)\n",
    "        train_loss.append(metrics[0])\n",
    "        train_mape.append(metrics[1])\n",
    "        train_rmse.append(metrics[2])\n",
    "        if iter % 50 == 0 :\n",
    "            log = 'Iter: {:03d}, Train Loss: {:.4f}, Train MAPE: {:.4f}, Train RMSE: {:.4f}'\n",
    "            print(log.format(iter, train_loss[-1], train_mape[-1], train_rmse[-1]),flush=True)\n",
    "    t2 = time.time()\n",
    "    train_time.append(t2-t1)\n",
    "    #validation\n",
    "    valid_loss = []\n",
    "    valid_mape = []\n",
    "    valid_rmse = []\n",
    "\n",
    "\n",
    "    s1 = time.time()\n",
    "    for iter, (x, y) in enumerate(valloader.get_iterator()):\n",
    "        testx = torch.Tensor(x).to(device)\n",
    "        testy = torch.Tensor(y).to(device)\n",
    "        metrics = engine.eval(testx, testy)\n",
    "        valid_loss.append(metrics[0])\n",
    "        valid_mape.append(metrics[1])\n",
    "        valid_rmse.append(metrics[2])\n",
    "    s2 = time.time()\n",
    "    log = 'Epoch: {:03d}, Inference Time: {:.4f} secs'\n",
    "    print(log.format(i,(s2-s1)))\n",
    "    val_time.append(s2-s1)\n",
    "    mtrain_loss = np.mean(train_loss)\n",
    "    mtrain_mape = np.mean(train_mape)\n",
    "    mtrain_rmse = np.mean(train_rmse)\n",
    "\n",
    "    mvalid_loss = np.mean(valid_loss)\n",
    "    mvalid_mape = np.mean(valid_mape)\n",
    "    mvalid_rmse = np.mean(valid_rmse)\n",
    "    his_loss.append(mvalid_loss)\n",
    "\n",
    "    log = 'Epoch: {:03d}, Train Loss: {:.4f}, Train MAPE: {:.4f}, Train RMSE: {:.4f}, Valid Loss: {:.4f}, Valid MAPE: {:.4f}, Valid RMSE: {:.4f}, Training Time: {:.4f}/epoch'\n",
    "    print(log.format(i, mtrain_loss, mtrain_mape, mtrain_rmse, mvalid_loss, mvalid_mape, mvalid_rmse, (t2 - t1)),flush=True)\n",
    "    torch.save(engine.model.state_dict(), \"./compare_with_state_of_art/graph_wavenet_model/epoch-\" + str(i) + \".pth\")\n",
    "print(\"Average Training Time: {:.4f} secs/epoch\".format(np.mean(train_time)))\n",
    "print(\"Average Inference Time: {:.4f} secs\".format(np.mean(val_time)))\n",
    "\n",
    "#testing\n",
    "bestid = np.argmin(his_loss)\n",
    "\n",
    "engine.model.load_state_dict(torch.load(\"./compare_with_state_of_art/graph_wavenet_model/epoch-\" + str(bestid+1) + \".pth\"))\n",
    "\n",
    "outputs = []\n",
    "realy = torch.Tensor(testloader['ys']).to(device)\n",
    "\n",
    "for iter, (x, y) in enumerate(testloader.get_iterator()):\n",
    "    testx = torch.Tensor(x).to(device)\n",
    "    with torch.no_grad():\n",
    "        preds = engine.model(testx)\n",
    "    outputs.append(preds)\n",
    "\n",
    "yhat = torch.cat(outputs,dim=0)\n",
    "yhat = yhat[:realy.size(0),...]\n",
    "\n",
    "\n",
    "print(\"Training finished\")\n",
    "print(\"The valid loss on best model is\", str(round(his_loss[bestid],4)))\n",
    "\n",
    "\n",
    "amae = []\n",
    "amape = []\n",
    "armse = []\n",
    "for i in range(12):\n",
    "    pred = scaler.inverse_transform(yhat[:,:,i])\n",
    "    real = realy[:,:,i]\n",
    "    metrics = util.metric(pred,real)\n",
    "    log = 'Evaluate best model on test data for horizon {:d}, Test MAE: {:.4f}, Test MAPE: {:.4f}, Test RMSE: {:.4f}'\n",
    "    print(log.format(i+1, metrics[0], metrics[1], metrics[2]))\n",
    "    amae.append(metrics[0])\n",
    "    amape.append(metrics[1])\n",
    "    armse.append(metrics[2])\n",
    "\n",
    "log = 'On average over 12 horizons, Test MAE: {:.4f}, Test MAPE: {:.4f}, Test RMSE: {:.4f}'\n",
    "print(log.format(np.mean(amae),np.mean(amape),np.mean(armse)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#testing\n",
    "bestid = np.argmin(his_loss)\n",
    "\n",
    "engine.model.load_state_dict(torch.load(\"./compare_with_state_of_art/graph_wavenet_model/epoch-\" + str(bestid+1) + \".pth\"))\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for iter, (x, y) in enumerate(testloader.get_iterator()):\n",
    "    testx = torch.Tensor(x).to(device)\n",
    "    with torch.no_grad():\n",
    "        preds = engine.model(testx)\n",
    "    outputs.append(preds)\n",
    "\n",
    "yhat = torch.cat(outputs,dim=0)\n",
    "pred = scaler.inverse_transform(yhat)    \n",
    "errors = compute_me(y_test, pred.cpu().numpy())\n",
    "print(errors[0], errors[1], errors[2])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.734826 13.919515 23.610320687294006\n"
     ]
    }
   ],
   "source": [
    "errors = compute_me(y_test, pred.cpu().numpy())\n",
    "print(errors[0], errors[1], errors[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## TaxiNY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63, 8856, 24) (63, 8856)\n"
     ]
    }
   ],
   "source": [
    "X_train_ny = np.load('./data/taxi_ny/X_train20.npy')\n",
    "y_train_ny = np.load('./data/taxi_ny/y_train20.npy')\n",
    "X_val_ny = np.load('./data/taxi_ny/X_val20.npy')\n",
    "y_val_ny = np.load('./data/taxi_ny/y_val20.npy')\n",
    "X_test_ny = np.load('./data/taxi_ny/X_test20.npy')\n",
    "y_test_ny = np.load('./data/taxi_ny/y_test20.npy')\n",
    "\n",
    "edges_in_ny = np.load('./data/taxi_ny/edges_manhattan.npy')\n",
    "ny_high_similar_poi = pd.read_csv('./data/taxi_ny/high_similar_poi.csv')\n",
    "\n",
    "print(X_train_ny.shape, y_train_ny.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8856, 1, 63, 24) (8856, 63) (1008, 1, 63, 24) (1008, 63)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = X_train_ny.transpose(1,0,2)[:,np.newaxis,:,:], y_train_ny.T\n",
    "X_val, y_val = X_val_ny.transpose(1,0,2)[:,np.newaxis,:,:], y_val_ny.T\n",
    "X_test, y_test = X_test_ny.transpose(1,0,2)[:,np.newaxis,:,:], y_test_ny.T\n",
    "\n",
    "scaler = StandardScaler(np.mean(X_train.flatten()), np.std(X_train.flatten()))\n",
    "\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_val_std = scaler.transform(X_val)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "print(X_train_std.shape, y_train.shape, X_test_std.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "adj_mx = np.zeros((63, 63))\n",
    "\n",
    "for i in range(63): \n",
    "    adj_mx[i,i] = 1\n",
    "    \n",
    "for i in range(len(edges_in_ny)):\n",
    "    adj_mx[edges_in_ny[i,0], edges_in_ny[i,1]] = 1\n",
    "    \n",
    "for i in range(len(high_similar_poi_sz)):\n",
    "    adj_mx[ny_high_similar_poi['i'].iloc[i], ny_high_similar_poi['j'].iloc[i]] = 1\n",
    "    \n",
    "adj_mx = torch.FloatTensor(adj_mx).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "trainloader = DataLoader(X_train_std, y_train, 1)\n",
    "valloader = DataLoader(X_val_std, y_val, 1)\n",
    "testloader = DataLoader(X_test_std, y_test, 1)\n",
    "    \n",
    "engine = trainer(scaler=scaler, in_dim=1, seq_length=24, num_nodes=63, nhid=32, dropout=0.5, lrate=0.01, wdecay=0.0001, \n",
    "    device=device, supports=[adj_mx], gcn_bool=True, addaptadj=True, aptinit=adj_mx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "Iter: 000, Train Loss: 41.8780, Train MAPE: inf, Train RMSE: 65.1953\n",
      "Iter: 050, Train Loss: 57.7694, Train MAPE: inf, Train RMSE: 76.7200\n",
      "Iter: 100, Train Loss: 10.0250, Train MAPE: inf, Train RMSE: 15.6046\n",
      "Iter: 150, Train Loss: 6.7972, Train MAPE: inf, Train RMSE: 7.9593\n",
      "Iter: 200, Train Loss: 13.6343, Train MAPE: inf, Train RMSE: 19.3703\n",
      "Iter: 250, Train Loss: 15.2930, Train MAPE: inf, Train RMSE: 23.1272\n",
      "Iter: 300, Train Loss: 31.1631, Train MAPE: inf, Train RMSE: 46.7367\n",
      "Iter: 350, Train Loss: 9.6679, Train MAPE: inf, Train RMSE: 13.4549\n",
      "Iter: 400, Train Loss: 10.1134, Train MAPE: inf, Train RMSE: 18.2023\n",
      "Iter: 450, Train Loss: 15.3073, Train MAPE: inf, Train RMSE: 27.7086\n",
      "Iter: 500, Train Loss: 16.3722, Train MAPE: inf, Train RMSE: 21.5574\n",
      "Iter: 550, Train Loss: 10.6068, Train MAPE: inf, Train RMSE: 16.0670\n",
      "Iter: 600, Train Loss: 18.0018, Train MAPE: inf, Train RMSE: 30.0206\n",
      "Iter: 650, Train Loss: 3.7110, Train MAPE: inf, Train RMSE: 4.6394\n",
      "Iter: 700, Train Loss: 12.2544, Train MAPE: inf, Train RMSE: 19.8851\n",
      "Iter: 750, Train Loss: 4.4382, Train MAPE: inf, Train RMSE: 5.7574\n",
      "Iter: 800, Train Loss: 11.4875, Train MAPE: inf, Train RMSE: 29.4427\n",
      "Iter: 850, Train Loss: 6.3182, Train MAPE: inf, Train RMSE: 14.2293\n",
      "Iter: 900, Train Loss: 10.8716, Train MAPE: inf, Train RMSE: 14.8795\n",
      "Iter: 950, Train Loss: 8.9346, Train MAPE: inf, Train RMSE: 12.2447\n",
      "Iter: 1000, Train Loss: 1.9396, Train MAPE: inf, Train RMSE: 3.2862\n",
      "Iter: 1050, Train Loss: 13.5557, Train MAPE: inf, Train RMSE: 29.8957\n",
      "Iter: 1100, Train Loss: 13.0019, Train MAPE: inf, Train RMSE: 20.9951\n",
      "Iter: 1150, Train Loss: 9.5084, Train MAPE: inf, Train RMSE: 16.0261\n",
      "Iter: 1200, Train Loss: 7.5220, Train MAPE: inf, Train RMSE: 8.7398\n",
      "Iter: 1250, Train Loss: 7.2695, Train MAPE: inf, Train RMSE: 9.4962\n",
      "Iter: 1300, Train Loss: 10.2926, Train MAPE: inf, Train RMSE: 25.9521\n",
      "Iter: 1350, Train Loss: 9.0028, Train MAPE: inf, Train RMSE: 14.3976\n",
      "Iter: 1400, Train Loss: 19.5341, Train MAPE: inf, Train RMSE: 35.0091\n",
      "Iter: 1450, Train Loss: 2.2617, Train MAPE: inf, Train RMSE: 3.0444\n",
      "Iter: 1500, Train Loss: 18.1386, Train MAPE: inf, Train RMSE: 23.5461\n",
      "Iter: 1550, Train Loss: 13.2127, Train MAPE: inf, Train RMSE: 21.8295\n",
      "Iter: 1600, Train Loss: 7.3472, Train MAPE: inf, Train RMSE: 9.5607\n",
      "Iter: 1650, Train Loss: 8.7052, Train MAPE: inf, Train RMSE: 12.0730\n",
      "Iter: 1700, Train Loss: 1.8679, Train MAPE: inf, Train RMSE: 3.6828\n",
      "Iter: 1750, Train Loss: 11.7240, Train MAPE: inf, Train RMSE: 20.4879\n",
      "Iter: 1800, Train Loss: 12.3446, Train MAPE: inf, Train RMSE: 19.3529\n",
      "Iter: 1850, Train Loss: 10.3700, Train MAPE: inf, Train RMSE: 15.7617\n",
      "Iter: 1900, Train Loss: 5.8276, Train MAPE: inf, Train RMSE: 7.7499\n",
      "Iter: 1950, Train Loss: 7.5130, Train MAPE: inf, Train RMSE: 11.4416\n",
      "Iter: 2000, Train Loss: 7.4019, Train MAPE: inf, Train RMSE: 10.9306\n",
      "Iter: 2050, Train Loss: 8.7974, Train MAPE: inf, Train RMSE: 15.4327\n",
      "Iter: 2100, Train Loss: 5.4685, Train MAPE: inf, Train RMSE: 8.7701\n",
      "Iter: 2150, Train Loss: 10.0388, Train MAPE: inf, Train RMSE: 15.0470\n",
      "Iter: 2200, Train Loss: 5.0106, Train MAPE: inf, Train RMSE: 6.5721\n",
      "Iter: 2250, Train Loss: 4.2236, Train MAPE: inf, Train RMSE: 5.4929\n",
      "Iter: 2300, Train Loss: 9.1712, Train MAPE: inf, Train RMSE: 16.6936\n",
      "Iter: 2350, Train Loss: 5.4535, Train MAPE: inf, Train RMSE: 7.7821\n",
      "Iter: 2400, Train Loss: 9.9193, Train MAPE: inf, Train RMSE: 15.0522\n",
      "Iter: 2450, Train Loss: 2.6559, Train MAPE: inf, Train RMSE: 4.1498\n",
      "Iter: 2500, Train Loss: 8.9836, Train MAPE: inf, Train RMSE: 12.0007\n",
      "Iter: 2550, Train Loss: 14.5455, Train MAPE: inf, Train RMSE: 18.5850\n",
      "Iter: 2600, Train Loss: 4.8895, Train MAPE: inf, Train RMSE: 7.6868\n",
      "Iter: 2650, Train Loss: 3.7738, Train MAPE: inf, Train RMSE: 5.0544\n",
      "Iter: 2700, Train Loss: 1.8594, Train MAPE: inf, Train RMSE: 2.5767\n",
      "Iter: 2750, Train Loss: 12.8959, Train MAPE: inf, Train RMSE: 21.9810\n",
      "Iter: 2800, Train Loss: 3.4341, Train MAPE: inf, Train RMSE: 5.8073\n",
      "Iter: 2850, Train Loss: 5.8829, Train MAPE: inf, Train RMSE: 8.1227\n",
      "Iter: 2900, Train Loss: 14.4562, Train MAPE: inf, Train RMSE: 28.0493\n",
      "Iter: 2950, Train Loss: 14.8838, Train MAPE: inf, Train RMSE: 20.2551\n",
      "Iter: 3000, Train Loss: 1.7880, Train MAPE: inf, Train RMSE: 2.5894\n",
      "Iter: 3050, Train Loss: 16.7366, Train MAPE: inf, Train RMSE: 26.9848\n",
      "Iter: 3100, Train Loss: 10.6751, Train MAPE: inf, Train RMSE: 17.0515\n",
      "Iter: 3150, Train Loss: 2.3867, Train MAPE: inf, Train RMSE: 3.5647\n",
      "Iter: 3200, Train Loss: 2.7731, Train MAPE: inf, Train RMSE: 4.1281\n",
      "Iter: 3250, Train Loss: 3.5698, Train MAPE: inf, Train RMSE: 5.5048\n",
      "Iter: 3300, Train Loss: 13.3074, Train MAPE: inf, Train RMSE: 20.8217\n",
      "Iter: 3350, Train Loss: 12.2755, Train MAPE: inf, Train RMSE: 21.4610\n",
      "Iter: 3400, Train Loss: 11.9389, Train MAPE: inf, Train RMSE: 19.5260\n",
      "Iter: 3450, Train Loss: 7.9109, Train MAPE: inf, Train RMSE: 13.4137\n",
      "Iter: 3500, Train Loss: 9.4108, Train MAPE: inf, Train RMSE: 13.0214\n",
      "Iter: 3550, Train Loss: 3.8686, Train MAPE: inf, Train RMSE: 5.1904\n",
      "Iter: 3600, Train Loss: 8.1394, Train MAPE: inf, Train RMSE: 10.9768\n",
      "Iter: 3650, Train Loss: 7.6284, Train MAPE: inf, Train RMSE: 10.8188\n",
      "Iter: 3700, Train Loss: 6.1461, Train MAPE: inf, Train RMSE: 8.3073\n",
      "Iter: 3750, Train Loss: 5.7192, Train MAPE: inf, Train RMSE: 8.0770\n",
      "Iter: 3800, Train Loss: 6.7331, Train MAPE: inf, Train RMSE: 9.0339\n",
      "Iter: 3850, Train Loss: 8.6954, Train MAPE: inf, Train RMSE: 13.3319\n",
      "Iter: 3900, Train Loss: 15.4786, Train MAPE: inf, Train RMSE: 21.5905\n",
      "Iter: 3950, Train Loss: 3.0810, Train MAPE: inf, Train RMSE: 4.0280\n",
      "Iter: 4000, Train Loss: 6.8114, Train MAPE: inf, Train RMSE: 10.7482\n",
      "Iter: 4050, Train Loss: 8.3037, Train MAPE: inf, Train RMSE: 12.9730\n",
      "Iter: 4100, Train Loss: 8.7101, Train MAPE: inf, Train RMSE: 15.7452\n",
      "Iter: 4150, Train Loss: 9.4122, Train MAPE: inf, Train RMSE: 15.8573\n",
      "Iter: 4200, Train Loss: 15.0210, Train MAPE: inf, Train RMSE: 22.5660\n",
      "Iter: 4250, Train Loss: 11.7525, Train MAPE: inf, Train RMSE: 17.4334\n",
      "Iter: 4300, Train Loss: 10.9473, Train MAPE: inf, Train RMSE: 18.9463\n",
      "Iter: 4350, Train Loss: 13.4778, Train MAPE: inf, Train RMSE: 18.2642\n",
      "Iter: 4400, Train Loss: 10.8386, Train MAPE: inf, Train RMSE: 18.2614\n",
      "Iter: 4450, Train Loss: 7.0199, Train MAPE: inf, Train RMSE: 10.5484\n",
      "Iter: 4500, Train Loss: 7.1072, Train MAPE: inf, Train RMSE: 13.4146\n",
      "Iter: 4550, Train Loss: 9.5284, Train MAPE: inf, Train RMSE: 17.3583\n",
      "Iter: 4600, Train Loss: 3.1034, Train MAPE: inf, Train RMSE: 5.7875\n",
      "Iter: 4650, Train Loss: 6.4061, Train MAPE: inf, Train RMSE: 10.2597\n",
      "Iter: 4700, Train Loss: 12.2290, Train MAPE: inf, Train RMSE: 20.3791\n",
      "Iter: 4750, Train Loss: 5.7406, Train MAPE: inf, Train RMSE: 9.5683\n",
      "Iter: 4800, Train Loss: 17.5387, Train MAPE: inf, Train RMSE: 27.0795\n",
      "Iter: 4850, Train Loss: 11.0371, Train MAPE: inf, Train RMSE: 17.9929\n",
      "Iter: 4900, Train Loss: 7.0406, Train MAPE: inf, Train RMSE: 10.9346\n",
      "Iter: 4950, Train Loss: 10.1117, Train MAPE: inf, Train RMSE: 15.3018\n",
      "Iter: 5000, Train Loss: 5.0370, Train MAPE: inf, Train RMSE: 5.5865\n",
      "Iter: 5050, Train Loss: 4.8866, Train MAPE: inf, Train RMSE: 8.1432\n",
      "Iter: 5100, Train Loss: 5.8667, Train MAPE: inf, Train RMSE: 8.2972\n",
      "Iter: 5150, Train Loss: 9.3521, Train MAPE: inf, Train RMSE: 14.2672\n",
      "Iter: 5200, Train Loss: 10.6713, Train MAPE: inf, Train RMSE: 16.1033\n",
      "Iter: 5250, Train Loss: 13.7355, Train MAPE: inf, Train RMSE: 27.8493\n",
      "Iter: 5300, Train Loss: 5.5371, Train MAPE: inf, Train RMSE: 7.6958\n",
      "Iter: 5350, Train Loss: 9.1661, Train MAPE: inf, Train RMSE: 14.6108\n",
      "Iter: 5400, Train Loss: 10.5093, Train MAPE: inf, Train RMSE: 17.3900\n",
      "Iter: 5450, Train Loss: 12.9568, Train MAPE: inf, Train RMSE: 20.5409\n",
      "Iter: 5500, Train Loss: 10.8955, Train MAPE: inf, Train RMSE: 18.3173\n",
      "Iter: 5550, Train Loss: 8.6346, Train MAPE: inf, Train RMSE: 14.4413\n",
      "Iter: 5600, Train Loss: 4.7924, Train MAPE: inf, Train RMSE: 7.1001\n",
      "Iter: 5650, Train Loss: 11.4004, Train MAPE: inf, Train RMSE: 17.1750\n",
      "Iter: 5700, Train Loss: 7.2006, Train MAPE: inf, Train RMSE: 15.3496\n",
      "Iter: 5750, Train Loss: 9.1917, Train MAPE: inf, Train RMSE: 17.5547\n",
      "Iter: 5800, Train Loss: 3.8352, Train MAPE: inf, Train RMSE: 5.8927\n",
      "Iter: 5850, Train Loss: 2.8536, Train MAPE: inf, Train RMSE: 3.8493\n",
      "Iter: 5900, Train Loss: 10.9164, Train MAPE: inf, Train RMSE: 20.1623\n",
      "Iter: 5950, Train Loss: 14.1784, Train MAPE: inf, Train RMSE: 20.5606\n",
      "Iter: 6000, Train Loss: 9.2971, Train MAPE: inf, Train RMSE: 15.4897\n",
      "Iter: 6050, Train Loss: 16.9788, Train MAPE: inf, Train RMSE: 26.3210\n",
      "Iter: 6100, Train Loss: 14.6993, Train MAPE: inf, Train RMSE: 26.6538\n",
      "Iter: 6150, Train Loss: 3.6721, Train MAPE: inf, Train RMSE: 7.0032\n",
      "Iter: 6200, Train Loss: 7.2662, Train MAPE: inf, Train RMSE: 11.3591\n",
      "Iter: 6250, Train Loss: 6.1108, Train MAPE: inf, Train RMSE: 8.7150\n",
      "Iter: 6300, Train Loss: 11.5356, Train MAPE: inf, Train RMSE: 18.1660\n",
      "Iter: 6350, Train Loss: 10.9476, Train MAPE: inf, Train RMSE: 16.9378\n",
      "Iter: 6400, Train Loss: 11.7296, Train MAPE: inf, Train RMSE: 15.7448\n",
      "Iter: 6450, Train Loss: 2.3296, Train MAPE: inf, Train RMSE: 3.0203\n",
      "Iter: 6500, Train Loss: 10.7505, Train MAPE: inf, Train RMSE: 18.7282\n",
      "Iter: 6550, Train Loss: 9.1647, Train MAPE: inf, Train RMSE: 13.8038\n",
      "Iter: 6600, Train Loss: 8.5814, Train MAPE: inf, Train RMSE: 12.3847\n",
      "Iter: 6650, Train Loss: 6.3598, Train MAPE: inf, Train RMSE: 10.0858\n",
      "Iter: 6700, Train Loss: 8.3824, Train MAPE: inf, Train RMSE: 14.4254\n",
      "Iter: 6750, Train Loss: 8.4404, Train MAPE: inf, Train RMSE: 12.2759\n",
      "Iter: 6800, Train Loss: 5.6838, Train MAPE: inf, Train RMSE: 8.4784\n",
      "Iter: 6850, Train Loss: 10.0571, Train MAPE: inf, Train RMSE: 17.0438\n",
      "Iter: 6900, Train Loss: 7.6865, Train MAPE: inf, Train RMSE: 11.3664\n",
      "Iter: 6950, Train Loss: 10.8823, Train MAPE: inf, Train RMSE: 15.1346\n",
      "Iter: 7000, Train Loss: 2.1117, Train MAPE: inf, Train RMSE: 2.8635\n",
      "Iter: 7050, Train Loss: 12.3633, Train MAPE: inf, Train RMSE: 19.1252\n",
      "Iter: 7100, Train Loss: 9.3593, Train MAPE: inf, Train RMSE: 12.6031\n",
      "Iter: 7150, Train Loss: 8.9109, Train MAPE: inf, Train RMSE: 13.9673\n",
      "Iter: 7200, Train Loss: 8.2277, Train MAPE: inf, Train RMSE: 12.7854\n",
      "Iter: 7250, Train Loss: 8.0840, Train MAPE: inf, Train RMSE: 13.1646\n",
      "Iter: 7300, Train Loss: 6.0539, Train MAPE: inf, Train RMSE: 8.6018\n",
      "Iter: 7350, Train Loss: 7.6766, Train MAPE: inf, Train RMSE: 19.7557\n",
      "Iter: 7400, Train Loss: 5.8940, Train MAPE: inf, Train RMSE: 8.0916\n",
      "Iter: 7450, Train Loss: 4.6812, Train MAPE: inf, Train RMSE: 7.7465\n",
      "Iter: 7500, Train Loss: 11.0606, Train MAPE: inf, Train RMSE: 23.2861\n",
      "Iter: 7550, Train Loss: 6.1756, Train MAPE: inf, Train RMSE: 10.0235\n",
      "Iter: 7600, Train Loss: 6.4446, Train MAPE: inf, Train RMSE: 9.6623\n",
      "Iter: 7650, Train Loss: 11.6917, Train MAPE: inf, Train RMSE: 19.7781\n",
      "Iter: 7700, Train Loss: 14.1915, Train MAPE: inf, Train RMSE: 24.1447\n",
      "Iter: 7750, Train Loss: 9.3700, Train MAPE: inf, Train RMSE: 13.8664\n",
      "Iter: 7800, Train Loss: 2.4087, Train MAPE: inf, Train RMSE: 3.2190\n",
      "Iter: 7850, Train Loss: 2.5818, Train MAPE: inf, Train RMSE: 3.2790\n",
      "Iter: 7900, Train Loss: 10.3229, Train MAPE: inf, Train RMSE: 15.5222\n",
      "Iter: 7950, Train Loss: 2.8598, Train MAPE: inf, Train RMSE: 3.5221\n",
      "Iter: 8000, Train Loss: 2.9966, Train MAPE: inf, Train RMSE: 4.6810\n",
      "Iter: 8050, Train Loss: 8.7949, Train MAPE: inf, Train RMSE: 13.2860\n",
      "Iter: 8100, Train Loss: 8.5064, Train MAPE: inf, Train RMSE: 14.9581\n",
      "Iter: 8150, Train Loss: 10.7916, Train MAPE: inf, Train RMSE: 17.1190\n",
      "Iter: 8200, Train Loss: 9.4135, Train MAPE: inf, Train RMSE: 13.8273\n",
      "Iter: 8250, Train Loss: 3.2725, Train MAPE: inf, Train RMSE: 6.0578\n",
      "Iter: 8300, Train Loss: 8.8138, Train MAPE: inf, Train RMSE: 13.2492\n",
      "Iter: 8350, Train Loss: 6.5743, Train MAPE: inf, Train RMSE: 9.4409\n",
      "Iter: 8400, Train Loss: 5.1114, Train MAPE: inf, Train RMSE: 8.0645\n",
      "Iter: 8450, Train Loss: 9.2735, Train MAPE: inf, Train RMSE: 12.5071\n",
      "Iter: 8500, Train Loss: 3.2425, Train MAPE: inf, Train RMSE: 4.3493\n",
      "Iter: 8550, Train Loss: 12.3444, Train MAPE: inf, Train RMSE: 20.7940\n",
      "Iter: 8600, Train Loss: 10.1769, Train MAPE: inf, Train RMSE: 15.8665\n",
      "Iter: 8650, Train Loss: 6.8853, Train MAPE: inf, Train RMSE: 11.0575\n",
      "Iter: 8700, Train Loss: 8.0032, Train MAPE: inf, Train RMSE: 13.1299\n",
      "Iter: 8750, Train Loss: 7.8315, Train MAPE: inf, Train RMSE: 14.2962\n",
      "Iter: 8800, Train Loss: 2.3096, Train MAPE: inf, Train RMSE: 4.7415\n",
      "Iter: 8850, Train Loss: 5.3444, Train MAPE: inf, Train RMSE: 7.8701\n",
      "Epoch: 001, Inference Time: 67.6622 secs\n",
      "Epoch: 001, Train Loss: 9.7892, Train MAPE: inf, Train RMSE: 15.3724, Valid Loss: 7.0897, Valid MAPE: 0.3277, Valid RMSE: 11.3085, Training Time: 3259.5447/epoch\n",
      "Average Training Time: 3259.5447 secs/epoch\n",
      "Average Inference Time: 67.6622 secs\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './compare_with_state_of_art/graph_wavenet_model/NY-epoch-1.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-4b6601e1713f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[0mbestid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhis_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m \u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./compare_with_state_of_art/graph_wavenet_model/NY-epoch-\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbestid\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".pth\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    582\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './compare_with_state_of_art/graph_wavenet_model/NY-epoch-1.pth'"
     ]
    }
   ],
   "source": [
    "print(\"start training...\",flush=True)\n",
    "his_loss =[]\n",
    "val_time = []\n",
    "train_time = []\n",
    "for i in range(1,2):\n",
    "    #if i % 10 == 0:\n",
    "        #lr = max(0.000002,args.learning_rate * (0.1 ** (i // 10)))\n",
    "        #for g in engine.optimizer.param_groups:\n",
    "            #g['lr'] = lr\n",
    "    train_loss = []\n",
    "    train_mape = []\n",
    "    train_rmse = []\n",
    "    t1 = time.time()\n",
    "    trainloader.shuffle()\n",
    "    for iter, (x, y) in enumerate(trainloader.get_iterator()):\n",
    "        trainx = torch.Tensor(x).to(device)\n",
    "        trainy = torch.Tensor(y).to(device)\n",
    "        metrics = engine.train(trainx, trainy)\n",
    "        train_loss.append(metrics[0])\n",
    "        train_mape.append(metrics[1])\n",
    "        train_rmse.append(metrics[2])\n",
    "        if iter % 50 == 0 :\n",
    "            log = 'Iter: {:03d}, Train Loss: {:.4f}, Train MAPE: {:.4f}, Train RMSE: {:.4f}'\n",
    "            print(log.format(iter, train_loss[-1], train_mape[-1], train_rmse[-1]),flush=True)\n",
    "    t2 = time.time()\n",
    "    train_time.append(t2-t1)\n",
    "    #validation\n",
    "    valid_loss = []\n",
    "    valid_mape = []\n",
    "    valid_rmse = []\n",
    "\n",
    "\n",
    "    s1 = time.time()\n",
    "    for iter, (x, y) in enumerate(valloader.get_iterator()):\n",
    "        testx = torch.Tensor(x).to(device)\n",
    "        testy = torch.Tensor(y).to(device)\n",
    "        metrics = engine.eval(testx, testy)\n",
    "        valid_loss.append(metrics[0])\n",
    "        valid_mape.append(metrics[1])\n",
    "        valid_rmse.append(metrics[2])\n",
    "    s2 = time.time()\n",
    "    log = 'Epoch: {:03d}, Inference Time: {:.4f} secs'\n",
    "    print(log.format(i,(s2-s1)))\n",
    "    val_time.append(s2-s1)\n",
    "    mtrain_loss = np.mean(train_loss)\n",
    "    mtrain_mape = np.mean(train_mape)\n",
    "    mtrain_rmse = np.mean(train_rmse)\n",
    "\n",
    "    mvalid_loss = np.mean(valid_loss)\n",
    "    mvalid_mape = np.mean(valid_mape)\n",
    "    mvalid_rmse = np.mean(valid_rmse)\n",
    "    his_loss.append(mvalid_loss)\n",
    "\n",
    "    log = 'Epoch: {:03d}, Train Loss: {:.4f}, Train MAPE: {:.4f}, Train RMSE: {:.4f}, Valid Loss: {:.4f}, Valid MAPE: {:.4f}, Valid RMSE: {:.4f}, Training Time: {:.4f}/epoch'\n",
    "    print(log.format(i, mtrain_loss, mtrain_mape, mtrain_rmse, mvalid_loss, mvalid_mape, mvalid_rmse, (t2 - t1)),flush=True)\n",
    "    torch.save(engine.model.state_dict(), \"./compare_with_state_of_art/graph_wavenet_model/epoch-\" + str(i) + \".pth\")\n",
    "print(\"Average Training Time: {:.4f} secs/epoch\".format(np.mean(train_time)))\n",
    "print(\"Average Inference Time: {:.4f} secs\".format(np.mean(val_time)))\n",
    "\n",
    "#testing\n",
    "bestid = np.argmin(his_loss)\n",
    "\n",
    "engine.model.load_state_dict(torch.load(\"./compare_with_state_of_art/graph_wavenet_model/epoch-\" + str(bestid+1) + \".pth\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.641262 9.229001 21.61650061607361\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "bestid = np.argmin(his_loss)\n",
    "\n",
    "engine.model.load_state_dict(torch.load(\"./compare_with_state_of_art/graph_wavenet_model/epoch-\" + str(bestid+1) + \".pth\"))\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for iter, (x, y) in enumerate(testloader.get_iterator()):\n",
    "    testx = torch.Tensor(x).to(device)\n",
    "    with torch.no_grad():\n",
    "        preds = engine.model(testx)\n",
    "    outputs.append(preds)\n",
    "\n",
    "yhat = torch.cat(outputs,dim=0)\n",
    "pred = scaler.inverse_transform(yhat)    \n",
    "errors = compute_me(y_test, pred.cpu().numpy())\n",
    "print(errors[0], errors[1], errors[2])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
